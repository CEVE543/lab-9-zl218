---
title: "CEVE 543 Fall 2025 Lab 9: Weather Typing with Hidden Markov Models"
subtitle: "Temporal clustering of synoptic patterns using HMMs"
author: Zijie (Ian) Liang
date: "2025-11-07"
type: "lab"
module: 2
week: 12
objectives:
  - "Apply Hidden Markov Models to atmospheric circulation data"
  - "Understand temporal dependencies in weather patterns"
  - "Compare HMM and K-means clustering approaches"
  - "Generate synthetic weather sequences"
ps_connection: "Advanced weather typing methods for PS2 Part 2"

engine: julia

format:
  html:
    toc: true
    toc-depth: 2
    code-block-bg: "#f8f8f8"
    code-block-border-left: "#e1e4e5"
    theme: simplex
    number-sections: true
    fig-format: png
  typst:
    fontsize: 11pt
    margin:
      x: 1in
      y: 1in
    number-sections: true
    fig-format: png

execute:
  cache: true
  freeze: auto

# Code formatting options
code-overflow: wrap
code-line-numbers: false
code-block-font-size: "0.85em"
---

## Background and Reading

This lab extends the weather typing analysis from Lab 8, replacing K-means clustering with Hidden Markov Models (HMMs).
While K-means treats each day independently, HMMs explicitly model temporal dependencies between consecutive weather patterns.
This is crucial for understanding weather persistence and transitions between regimes.

### Hidden Markov Models

An HMM consists of:

- **Hidden states**: Unobserved weather regimes (what we want to identify)
- **Observations**: PC scores we actually measure
- **Emission distributions**: How each state generates observations (multivariate normal in PC space)
- **Transition matrix**: Probabilities of switching between states from one day to the next
- **Initial distribution**: Probability of starting in each state

## Code Setup

Begin with project management:

```{julia}
#| output: false
using Pkg
lab_dir = dirname(@__FILE__)
Pkg.activate(lab_dir)
# Pkg.instantiate() # uncomment this the first time you run the lab to install packages, then comment it back
```

Load all required packages:

```{julia}
using YAXArrays
using NetCDF
using CairoMakie
using GeoMakie
using MultivariateStats
using Statistics
using LaTeXStrings
using DimensionalData
using NaturalEarth
using HiddenMarkovModels
using Distributions
using Random
using Dates
using LinearAlgebra: diag
using StatsBase
```

It's always good practice to set a random seed for reproducibility:

```{julia}
Random.seed!(543)
```

Define plotting helpers and colormaps:

```{julia}
#| output: false
cmap_absolute = :plasma
cmap_diverging = :PuOr

function diverging_colormap(data, colormap=:PuOr)
    max_abs = maximum(abs.(skipmissing(data)))
    return (colormap, (-max_abs, max_abs))
end

function create_geoaxis(fig, position; title="", kwargs...)
    return GeoAxis(fig[position...],
        title=title,
        dest="+proj=latlong +datum=WGS84",
        xgridvisible=false,
        ygridvisible=false,
        xticksvisible=false,
        yticksvisible=false,
        xticklabelsvisible=false,
        yticklabelsvisible=false;
        kwargs...)
end

# Load country boundaries for plotting
countries = naturalearth("admin_0_countries", 110)
```

## Data Loading and PCA

We consolidate the data loading and PCA into a single comprehensive function.
This function handles all preprocessing steps from Lab 8:

```{julia}
#| output: false
"""
	prepare_pca_data(data_file; n_pcs=4)

Load streamfunction data, calculate anomalies, and perform PCA.

Returns a named tuple with:
- ψ_climatology: Mean streamfunction field
- ψ_anomaly: Anomaly fields (3D YAXArray)
- pc_scores: PC time series (ntimes × n_pcs)
- eof_patterns: Spatial patterns (reshaped for plotting)
- pca_model: Fitted PCA model
- explained_var_ratio: Variance explained by each PC
- coordinates: (lon, lat, time) for plotting
"""
function prepare_pca_data(data_file; n_pcs=4)
    # Load dataset
    ds = open_dataset(data_file)
    ψ = ds[:streamfunction] ./ 1e6

    # Convert longitude from 0-360 to -180-180
    lon_values = Array(ψ.lon)
    lon_converted = ifelse.(lon_values .> 180, lon_values .- 360, lon_values)
    ψ = DimensionalData.set(ψ, :lon => lon_converted)

    # Load into memory
    ψ = readcubedata(ψ)

    # Calculate climatology and anomalies
    ψ_climatology = mean(ψ, dims=:time)[:, :, 1]
    ψ_anomaly = ψ .- ψ_climatology

    # Convert to array and reshape for PCA
    ψ_array = Array(ψ_anomaly)
    nlons, nlats, ntimes = size(ψ_array)
    ψ_matrix = reshape(ψ_array, (nlons * nlats, ntimes))

    # Fit PCA model
    pca_model = fit(PCA, ψ_matrix; maxoutdim=n_pcs)

    # Extract PC scores and EOFs
    pc_scores = MultivariateStats.transform(pca_model, ψ_matrix)'
    eof_patterns_flat = projection(pca_model)

    # Calculate explained variance
    explained_var = principalvars(pca_model)
    total_var = var(pca_model)
    explained_var_ratio = explained_var ./ total_var

    # Reshape EOFs for plotting
    eof_patterns = [reshape(eof_patterns_flat[:, i], (nlons, nlats)) for i in 1:n_pcs]

    # Store coordinates
    coordinates = (
        lon=Array(ψ_climatology.lon),
        lat=Array(ψ_climatology.lat),
        time=Array(lookup(ψ_anomaly, :time)),
    )

    return (
        ψ_climatology=ψ_climatology,
        ψ_anomaly=ψ_anomaly,
        pc_scores=pc_scores,
        eof_patterns=eof_patterns,
        pca_model=pca_model,
        explained_var_ratio=explained_var_ratio,
        coordinates=coordinates,
    )
end
```

Load and process the data.
We use the same NCEP streamfunction data from Lab 8:

```{julia}
#| output: false
streamfunction_file = joinpath(lab_dir, "ncep_streamfunction_regional_1979_2025.nc")
pca_data = prepare_pca_data(streamfunction_file; n_pcs=4)
```

Create helper function for adding boundaries:

```{julia}
#| output: false
lon_lims = extrema(pca_data.coordinates.lon)
lat_lims = extrema(pca_data.coordinates.lat)

function add_boundaries!(ax)
    poly!(ax,
        GeoMakie.to_multipoly(countries.geometry);
        color=:transparent,
        strokecolor=:black,
        strokewidth=1.5,
    )
    xlims!(ax, lon_lims...)
    ylims!(ax, lat_lims...)
end
```

Thus far, everything should look the same as Lab 8.
You can visualize the EOF patterns and explained variance as before if you wish.

## Hidden Markov Model Implementation

HMMs differ fundamentally from K-means by modeling temporal dependencies.
While K-means asks "what spatial patterns exist?", HMMs ask "how do weather patterns evolve in time?"
The Baum-Welch algorithm learns both the emission distributions (where states live in PC space) and the transition probabilities (how states evolve day-to-day).
This means HMM state assignments consider temporal context, not just instantaneous PC values.

### Prepare Data as Multiple Sequences

HMMs always want to work with sequences.
We split the data into separate NDJF (Nov-Feb) seasons, treating each as an independent sequence.
For more, see [the docs](https://gdalle.github.io/HiddenMarkovModels.jl/stable/examples/basics/#Multiple-sequences)

```{julia}
function split_into_seasons(pc_scores, times)
    sequences = Vector{Vector{Vector{Float64}}}()

    # Get unique years
    years = unique(year.(times))

    for y in years
        # NDJF spans two calendar years (Nov-Dec of y, Jan-Feb of y+1)
        season_mask = ((year.(times) .== y) .& (month.(times) .>= 11)) .|
                      ((year.(times) .== y + 1) .& (month.(times) .<= 2))

        if sum(season_mask) > 0
            season_pcs = pc_scores[season_mask, :]
            # Convert each row to a vector and collect into a vector of vectors
            season_obs = [vec(season_pcs[i, :]) for i in 1:size(season_pcs, 1)]
            push!(sequences, season_obs)
        end
    end

    # Concatenate all sequences
    obs_concat = reduce(vcat, sequences)

    # Calculate sequence end indices
    seq_ends = cumsum(length.(sequences))

    return obs_concat, seq_ends
end

obs_concat, seq_ends = split_into_seasons(pca_data.pc_scores, pca_data.coordinates.time)
println("Number of NDJF seasons: $(length(seq_ends))")
println("Total observations: $(length(obs_concat))")
println("Example season length: $(seq_ends[1]) days")
```

### Initialize and Train HMM

Before we can train an HMM, we need to specify its structure.
An HMM has three components: 

1. the initial state distribution (what state does a sequence start in?)
2. the transition matrix (how do states evolve from one day to the next?), and
3.  the emission distributions (what observations does each state generate?).

The Baum-Welch algorithm will learn all of these from data, but we need to provide starting values.
A key modeling choice is what distribution to use for emissions.
We'll use multivariate normal distributions in PC space.
The multivariate normal has nice properties: it's fully characterized by its mean vector and covariance matrix, and it has closed-form update equations in the Baum-Welch algorithm.
The mean vector defines the "center" of each weather regime in PC space, while the covariance captures how much variability there is around that center.

For the covariance structure, we have a choice between full covariance matrices (allowing correlations between different PCs within a state) or diagonal covariance matrices (assuming PCs vary independently within each state).
We'll use diagonal covariances for several reasons.
First, PCs are orthogonal by construction across the entire dataset, so there's little reason to expect strong correlations within individual states.
Second, diagonal covariances reduce the number of parameters from $\mathcal{O}(n_{pcs}^2)$ to $\mathcal{O}(n_{pcs})$ per state, making estimation more stable when data is limited.
Third, they're easier to interpret: each PC dimension has its own variance, telling us which modes are more or less variable within each regime.
We could try full covariance matrices later as an experiment, but diagonal is a sensible default.

For initialization, we'll use random starting values rather than trying to be clever.
We draw initial state probabilities uniformly at random and normalize them to sum to one.
We do the same for each row of the transition matrix, ensuring each row sums to one (these are conditional probabilities).
For the emission distributions, we start with mean vectors drawn from $\mathcal{N}(0, 1)$ and diagonal covariances with all variances set to 1.0.
These choices are arbitrary but reasonable: the Baum-Welch algorithm will find better values during training.

One caution: Baum-Welch is an expectation-maximization (EM) algorithm, which means it's sensitive to initialization.
Different random seeds can lead to different local optima.
More sophisticated initialization strategies exist (like using K-means cluster centers as initial emission means), but for now we'll keep it simple and just note that you might want to try multiple random seeds to check sensitivity.

```{julia}
#| output: false
function initialize_hmm(n_states, n_dims; seed=42)
    Random.seed!(seed)

    # Random initial state distribution (normalized)
    init_probs = rand(n_states)
    init_probs ./= sum(init_probs)

    # Random transition matrix (each row sums to 1)
    trans_probs = rand(n_states, n_states)
    trans_probs ./= sum(trans_probs, dims=2)

    # Random emission distributions (MV Normal with diagonal covariance)
    # Mean: random N(0,1), Variance: 1.0 for each dimension
    emissions = [MvNormal(randn(n_dims), ones(n_dims)) for _ in 1:n_states]

    return HMM(init_probs, trans_probs, emissions)
end

n_states = 6  # Following Doss-Gollin et al. (2018)
n_pcs = size(pca_data.pc_scores, 2)

hmm_init = initialize_hmm(n_states, n_pcs)
```

Train the HMM using Baum-Welch algorithm:

```{julia}
# Train HMM on all sequences
hmm_trained, loglikelihood_evolution = baum_welch(hmm_init, obs_concat; seq_ends=seq_ends, max_iterations=100, atol=1e-3)

println("Training complete!")
println("Final log-likelihood: $(round(loglikelihood_evolution[end], digits=2))")
```

Plot training convergence (skip the first iteration because it's off the chart bad)

```{julia}
let
    fig = Figure(size=(800, 500))
    ax = Axis(fig[1, 1],
        xlabel="Iteration",
        ylabel="Log-Likelihood",
        title="HMM Training Convergence",
    )
    lines!(ax, 2:length(loglikelihood_evolution), loglikelihood_evolution[2:end],
        color=:steelblue, linewidth=2,
    )
    fig
end
```

### Decode States with Viterbi Algorithm

Use the Viterbi algorithm to find the most likely state sequence:

```{julia}
# Decode all sequences at once
all_states, log_likelihoods = viterbi(hmm_trained, obs_concat; seq_ends=seq_ends)

println("State assignments for first 20 days:")
println(all_states[1:20])
println("\nNumber of sequences decoded: $(length(log_likelihoods))")
```

Visualize state assignments for one season:

```{julia}
let
    # Helper function to get sequence limits
    function seq_limits(seq_ends, idx)
        start_idx = idx == 1 ? 1 : seq_ends[idx-1] + 1
        end_idx = seq_ends[idx]
        return start_idx, end_idx
    end

    # Use first complete season (index 1)
    season_idx = 1
    start_idx, end_idx = seq_limits(seq_ends, season_idx)
    states = all_states[start_idx:end_idx]
    n_days = length(states)

    fig = Figure(size=(1200, 400))
    ax = Axis(fig[1, 1],
        xlabel="Day of Season",
        ylabel="Weather State",
        title="State Assignments for NDJF Season $(season_idx)",
        yticks=1:n_states)

    # Plot as step function
    stairs!(ax, 1:n_days, states, color=:steelblue, linewidth=2)

    fig
end
```

## Understanding HMM Components

### Transition Matrix

The transition matrix shows the probability of transitioning from one state to another.
Diagonal elements represent persistence (staying in the same state).

```{julia}
let
    trans_matrix = transition_matrix(hmm_trained)

    fig = Figure(size=(800, 700))
    ax = Axis(fig[1, 1],
        xlabel="To State",
        ylabel="From State",
        title="Transition Probability Matrix",
        xticks=1:n_states,
        yticks=1:n_states,
        aspect=DataAspect())

    hm = heatmap!(ax, trans_matrix, colormap=:Blues, colorrange=(0, 1))

    # Add text annotations
    for i in 1:n_states
        for j in 1:n_states
            text!(ax, i, j,
                text=string(round(trans_matrix[i, j], digits=2)),
                align=(:center, :center),
                color=:black,
                fontsize=14)
        end
    end

    Colorbar(fig[1, 2], hm, label="Probability")
    fig
end
```

### Emission Distributions vs Composites

This is a key conceptual point: The emission distributions are multivariate normals in PC space, while composites are spatial averages.

First, examine the emission distribution parameters:

```{julia}
let
    obs_dists = obs_distributions(hmm_trained)
    println("Emission distribution parameters (mean in PC space):")
    for i in 1:n_states
        μ = mean(obs_dists[i])
        σ = sqrt.(diag(cov(obs_dists[i])))
        println("\nState $i:")
        println("  Mean: ", round.(μ, digits=2))
        println("  Std:  ", round.(σ, digits=2))
    end
end
```

Now create spatial maps in two different ways and compare them.
First, reconstruct maps from the emission distribution means (the learned PC weights).
Second, create composites by averaging all actual days assigned to each state:

```{julia}
let
    # Load full anomaly array
    ψ_array = Array(pca_data.ψ_anomaly)
    nlons, nlats, ntimes = size(ψ_array)

    # Method 1: Reconstruct from emission means (PC weights times EOF patterns)
    obs_dists = obs_distributions(hmm_trained)
    emission_maps = zeros(nlons, nlats, n_states)

    for k in 1:n_states
        μ = mean(obs_dists[k])  # Mean PC scores for this state
        # Use MultivariateStats.reconstruct to properly inverse transform
        ψ_reconstructed = MultivariateStats.reconstruct(pca_data.pca_model, μ)
        emission_maps[:, :, k] = reshape(ψ_reconstructed, nlons, nlats)
    end

    # Method 2: Composite from actual days
    # Filter to only NDJF seasons (matching the HMM training data)
    times = pca_data.coordinates.time
    years = unique(year.(times))
    ndjf_mask = falses(length(times))

    for y in years
        season_mask = ((year.(times) .== y) .& (month.(times) .>= 11)) .|
                      ((year.(times) .== y + 1) .& (month.(times) .<= 2))
        ndjf_mask .|= season_mask
    end

    ψ_ndjf = ψ_array[:, :, ndjf_mask]
    composites = zeros(nlons, nlats, n_states)

    for k in 1:n_states
        cluster_days = findall(all_states .== k)
        if length(cluster_days) > 0
            cluster_data = ψ_ndjf[:, :, cluster_days]
            composites[:, :, k] = mean(cluster_data, dims=3)[:, :, 1]
        end
    end

    # Determine color ranges separately
    # Emission means are in PCA-reconstructed space
    # Composites are in original anomaly space
    crange_emission = maximum(abs.(extrema(emission_maps)))
    crange_emission = (-crange_emission, crange_emission)

    crange_composite = maximum(abs.(extrema(composites)))
    crange_composite = (-crange_composite, crange_composite)

    # Grid layout (3 columns, flexible rows)
    ncols = 3
    nrows = ceil(Int, n_states / ncols)

    # Plot 1: Emission mean reconstructions
    fig1 = Figure(size=(1200, 400 * nrows))

    for k in 1:n_states
        row = div(k - 1, ncols) + 1
        col = mod(k - 1, ncols) + 1

        ax = create_geoaxis(fig1, (row, col); title="State $k: Emission Mean")
        heatmap!(ax, pca_data.coordinates.lon, pca_data.coordinates.lat,
            emission_maps[:, :, k]; colormap=cmap_diverging, colorrange=crange_emission)
        add_boundaries!(ax)
    end

    Colorbar(fig1[:, ncols+1], limits=crange_emission, colormap=cmap_diverging,
        label=L"$\psi$ Anomaly ($10^6$ m$^2$/s)")

    fig1
end
```

```{julia}
let
    # Load full anomaly array
    ψ_array = Array(pca_data.ψ_anomaly)
    nlons, nlats, ntimes = size(ψ_array)

    # Filter to NDJF seasons (matching HMM training data)
    times = pca_data.coordinates.time
    years = unique(year.(times))
    ndjf_mask = falses(length(times))

    for y in years
        season_mask = ((year.(times) .== y) .& (month.(times) .>= 11)) .|
                      ((year.(times) .== y + 1) .& (month.(times) .<= 2))
        ndjf_mask .|= season_mask
    end

    ψ_ndjf = ψ_array[:, :, ndjf_mask]

    # Composite from actual days
    composites = zeros(nlons, nlats, n_states)

    for k in 1:n_states
        cluster_days = findall(all_states .== k)
        if length(cluster_days) > 0
            cluster_data = ψ_ndjf[:, :, cluster_days]
            composites[:, :, k] = mean(cluster_data, dims=3)[:, :, 1]
        end
    end

    # Color range based on composites
    crange = maximum(abs.(extrema(composites)))
    crange = (-crange, crange)

    # Grid layout
    ncols = 3
    nrows = ceil(Int, n_states / ncols)

    # Plot 2: Composites
    fig2 = Figure(size=(1200, 400 * nrows))

    for k in 1:n_states
        row = div(k - 1, ncols) + 1
        col = mod(k - 1, ncols) + 1

        ax = create_geoaxis(fig2, (row, col);
            title="State $k: Composite (n=$(sum(all_states .== k)))")
        heatmap!(ax, pca_data.coordinates.lon, pca_data.coordinates.lat,
            composites[:, :, k]; colormap=cmap_diverging, colorrange=crange)
        add_boundaries!(ax)
    end

    Colorbar(fig2[:, ncols+1], limits=crange, colormap=cmap_diverging,
        label=L"$\psi$ Anomaly ($10^6$ m$^2$/s)")

    fig2
end
```

::: {.callout-note}
## Key Distinction

The **emission distributions** define probability densities in 4D PC space (multivariate normals).
The **composite maps** show the average circulation pattern for all days assigned to each state.

While related, these are different representations:

- Emission distributions tell us the typical PC values for each state
- Composites show the typical spatial pattern in the original data space
- Days assigned to a state have PC values drawn from the emission distribution
- The composite is the average of projecting those PC values back to physical space
:::

## Analysis Questions

K-means assigns each day to its nearest cluster center in PC space, treating all days independently.
HMMs learn both where states live in PC space (emission distributions) AND how they transition over time (transition matrix).
Even with random initialization, Baum-Welch discovers temporal structure if it exists in the data.
The HMM might assign a day to a "suboptimal" state (in terms of PC distance alone) if doing so makes the temporal sequence more probable overall.
K-means cannot do this - it fundamentally ignores temporal order.

1. Fit HMMs with 3, 4, 5, and 7 states. How does the transition matrix structure change? Is temporal persistence stronger with fewer or more states?

*When I fit `HMMs` with 3 or 4 states, the transition probability matrices show very strong diagonal dominance. The diagonal entries, probability of staying in the same state, are typically high (often 0.6–0.9 depending on the fit). Each state is persistent and tends to last for multiple consecutive days. With fewer states, each state represents a broad synoptic regime, so the model has little reason to switch frequently. As I increase the number of states to 5 or 7, the diagonal elements weaken, and off-diagonal transition probabilities become more prominent. The model begins to represent finer-scale sub-regimes, which are naturally less persistent. In these cases, states switch more frequently, and certain pairs of states may show preferred transition pathways.*

2. Run the same HMM configuration (6 states, 4 PCs) with 3 different random seeds. How different are the resulting state patterns and transition matrices?

*When the model run the same 6-state `HMM` with different random seeds, the overall structure is similar but some details shift. The broad synoptic regimes usually appear in all three runs since they are strongly supported by the PC data. However, the exact spatial patterns and PC means can rotate or reorder.The transition matrices also maintain the same qualitative shape: a few states show high persistence, and certain pairs of states recur as common transitions. However, the exact transition probabilities can vary noticeably between seeds especially for states that occur less frequently.*

3. Compare results to the $K$-means clustering from lab 8. Where do they agree and disagree? Why?

*`K-means` and `HMM` generally agree on the spatial patterns of the weather regimes. Because both methods cluster in the same PC space, they tend to identify similar large-scale circulation types. For example, both methods pick out the strong cyclonic regime and the more zonal, weaker-flow regime. They disagree in how days are assigned to those regimes. `K-means` treats each day independently, so the sequence of assigned weather types can jump rapidly from one regime to another. The `HMM`, by contrast, includes a transition matrix that enforces temporal persistence. The `HMM` produces longer and smoother periods where the atmosphere remains in the same state.*

4. Write code to generate synthetic weather sequences from your fitted HMM using the `rand()` function. Create a 120-day sequence with corresponding PC values and spatial maps for a few days. Compare state frequencies, transition frequencies, and PC distributions between real and synthetic data.

First, I set up some functions that good for used.
```{julia}
# Empirical transition matrix
function empirical_transition_matrix(states::Vector{Int}, K::Int)
    counts = zeros(Float64, K, K)
    for t in 1:length(states)-1
        i, j = states[t], states[t+1]
        counts[i, j] += 1
    end
    eps = 1e-12
    row_sums = sum(counts, dims=2) .+ eps
    return counts ./ row_sums
end

# Fallback simulator

function simulate_hmm(hmm, T::Int)
    π  = initialization(hmm)          # initial state probabilities
    A  = transition_matrix(hmm)       # K×K transition matrix
    Ds = obs_distributions(hmm)       # vector of emission distributions (one per state)

    K = length(π)
    states = Vector{Int}(undef, T)
    states[1] = sample(1:K, Weights(π))
    for t in 2:T
        states[t] = sample(1:K, Weights(A[states[t-1], :]))
    end
    obs = [rand(Ds[states[t]]) for t in 1:T]
    return obs, states
end

# Unified entry point for simulation

function generate_synthetic_sequence(hmm, T::Int)
    sim = rand(hmm, T)  # try package simulator
    if isstructtype(typeof(sim)) && hasproperty(sim, :state_seq) && hasproperty(sim, :obs_seq)
        return sim.obs_seq, sim.state_seq
    else
        # Older versions may return only observations; use manual path
        return simulate_hmm(hmm, T)
    end
end

```


I created 120 dataset in sequence.
```{julia}
# Generate a 120-day synthetic sequence 
Random.seed!(543)
T_syn = 120
syn_obs, syn_states = generate_synthetic_sequence(hmm_trained, T_syn)

# Convert to T_syn × n_pcs matrix
syn_pc = reduce(vcat, (permutedims(obs) for obs in syn_obs))  # each obs is a row

println("Synthetic sequence length: ", size(syn_pc))
println("First 10 synthetic states: ", syn_states[1:min(10, length(syn_states))])

```

Then I conver the PC back to the the spatial map for selective days
```{julia}
# Reconstruct a few synthetic spatial maps 
sel_days = filter(d -> d ≤ T_syn, [1, 30, 60, 90])

# Use names that won't shadow YAXArrays.lon/lat
ψ_array = Array(pca_data.ψ_anomaly)
nlons, nlats, _ = size(ψ_array)
lons = pca_data.coordinates.lon
lats = pca_data.coordinates.lat

reconstructed_maps = Dict{Int, Array{Float64,2}}()
for d in sel_days
    # Ensure a plain Vector for reconstruct (safer than RowVector)
    μ = vec(syn_pc[d, :])
    ψ_vec = MultivariateStats.reconstruct(pca_data.pca_model, μ)  # back to flattened field
    reconstructed_maps[d] = reshape(ψ_vec, nlons, nlats)
end

```

Next, I tried to plot these anomaly maps
```{julia}
# Quick plot
let
    n = length(sel_days)
    ncols = min(n, 3)
    nrows = ceil(Int, n / ncols)
    vmax = maximum(abs, vcat([reconstructed_maps[d] for d in sel_days]...))
    crange = (-vmax, vmax)
    fig = Figure(size=(1000, 350*nrows))
    for (k, d) in enumerate(sel_days)
        r = div(k-1, ncols) + 1
        c = mod(k-1, ncols) + 1
        ax = create_geoaxis(fig, (r, c); title="Synthetic Day $d (State $(syn_states[d]))")
        heatmap!(ax, lons, lats, reconstructed_maps[d]; colormap=:PuOr, colorrange=crange)
        add_boundaries!(ax)
    end
    Colorbar(fig[:, ncols+1], label=L"$\psi$ Anomaly ($10^6$ m$^2$/s)")
    fig
end

```

Compared the real data and the synthetic data
```{julia}
# Real NDJF comparators
times = pca_data.coordinates.time
years = unique(year.(times))
ndjf_mask = falses(length(times))
for y in years
    season_mask = ((year.(times) .== y) .& (month.(times) .>= 11)) .|
                  ((year.(times) .== y + 1) .& (month.(times) .<= 2))
    ndjf_mask .|= season_mask
end
real_pc_ndjf = pca_data.pc_scores[ndjf_mask, :]  # nt×n_pcs (the training subset)
real_states_ndjf = all_states                    # your Viterbi states over NDJF

# Compare state frequencies 
n_states = maximum(real_states_ndjf)             # or pass your known K
real_state_freq = counts(real_states_ndjf, 1:n_states)
syn_state_freq  = counts(syn_states,       1:n_states)
println("\nState frequency (REAL NDJF):   ", real_state_freq)
println("State frequency (SYNTHETIC):   ", syn_state_freq)
```

Use the transition matrices to view the differences.
```{julia}
# Compare transition matrices
real_A_emp = empirical_transition_matrix(real_states_ndjf, n_states)
syn_A_emp  = empirical_transition_matrix(syn_states,       n_states)

println("\nEmpirical transition matrix (REAL):")
show(stdout, "text/plain", round.(real_A_emp, digits=3)); println()
println("\nEmpirical transition matrix (SYNTHETIC):")
show(stdout, "text/plain", round.(syn_A_emp, digits=3)); println()

let
    fig = Figure(size=(1100, 450))
    ax1 = Axis(fig[1, 1], title="REAL: Transition Probabilities", xlabel="To", ylabel="From",
               xticks=1:n_states, yticks=1:n_states, aspect=DataAspect())
    hm1 = heatmap!(ax1, real_A_emp, colormap=:Blues, colorrange=(0, 1))
    Colorbar(fig[1, 2], hm1, label="P")

    ax2 = Axis(fig[1, 3], title="SYNTHETIC: Transition Probabilities", xlabel="To", ylabel="From",
               xticks=1:n_states, yticks=1:n_states, aspect=DataAspect())
    hm2 = heatmap!(ax2, syn_A_emp, colormap=:Blues, colorrange=(0, 1))
    Colorbar(fig[1, 4], hm2, label="P")
    fig
end


```

Also, I compared the PC distributions with graphs.
```{julia}
n_pcs = size(real_pc_ndjf, 2)

let
    ncols = min(n_pcs, 2)
    nrows = ceil(Int, n_pcs / ncols)
    fig = Figure(size=(900, 320*nrows))

    
    step_xy = function (h::Histogram)
        edges = collect(h.edges[1])                        # bin edges (length nbins+1)
        weights = h.weights ./ max(sum(h.weights), 1)      # normalize to probability mass
        xs = vcat(edges[1], vec(repeat(edges[2:end], inner=2)), edges[end])
        ys = vcat(0.0, vec(repeat(weights, inner=2)), 0.0)
        return xs, ys
    end

    for j in 1:n_pcs
        r = div(j-1, ncols) + 1
        c = mod(j-1, ncols) + 1
        ax = Axis(fig[r, c], title="PC$j Distribution (REAL vs SYN)",
                  xlabel="PC$j", ylabel="Frequency (normalized)")

        # Ensure both vectors are Float64
        real_j = collect(Float64.(real_pc_ndjf[:, j]))
        syn_j  = collect(Float64.(syn_pc[:, j]))

        # Correct Histogram API: pass nbins as a keyword
        h_real = fit(Histogram, real_j; nbins=30, closed=:left)
        h_syn  = fit(Histogram, syn_j;  nbins=30, closed=:left)

        xs_r, ys_r = step_xy(h_real)
        xs_s, ys_s = step_xy(h_syn)

        # Draw step curves (no explicit colors)
        lines!(ax, xs_r, ys_r, linewidth=2, label="REAL")
        lines!(ax, xs_s, ys_s, linewidth=2, linestyle=:dash, label="SYN")
        axislegend(ax, position=:rt)
    end

    fig
end

```

Finally let me check the numberival differences.
```{julia}
# Numeric summaries
# Compare mean and std of each PC between REAL (NDJF) and SYN sequences.
println("\nPC means (REAL vs SYN):")
for j in 1:n_pcs
    real_j = Float64.(real_pc_ndjf[:, j])
    syn_j  = Float64.(syn_pc[:, j])

    m_real = mean(real_j); s_real = std(real_j)
    m_syn  = mean(syn_j);  s_syn  = std(syn_j)

    println(
        "PC$j: mean(real)=$(round(m_real, digits=2)), ",
        "std(real)=$(round(s_real, digits=2))  |  ",
        "mean(syn)=$(round(m_syn, digits=2)), ",
        "std(syn)=$(round(s_syn, digits=2))"
    )
end

```

5. Describe how you would extend this to predict rainfall. What emission distribution would you use? How would you validate whether HMM states relate to rainfall patterns? What are the fundamental limitations for extreme rainfall?

*To extend my `HMM` to rainfall, I would keep the `HMM` trained on the circulation PCs and then model rainfall as a state-dependent emission. For each hidden state, I would estimate the probability of a wet day and model positive rainfall amounts with a lognormal distribution. To validate the connection between states and rainfall, I would composite rainfall by state and check whether some states show consistently higher rainfall or higher wet-day frequency. Clear differences would indicate that the `HMM` states are meteorologically meaningful. The main limitation is that extreme rainfall often comes from localized convection not fully captured by large-scale PCs.*
